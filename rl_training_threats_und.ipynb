{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Agent for Training Num of Users for a Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preset Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'dataset/dual_s_data_mini.csv'\n",
    "users_low = 1\n",
    "users_res = 2\n",
    "users_high = 19\n",
    "#number_of_service = 2\n",
    "\n",
    "latency_threshold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. states: RAM(MB), #Cores, BG WL(%), GPU(MB), S1:Users, S2:Users\n",
    "\n",
    "2. More reward for the number closer to (s1,s2) with latency below given threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : (1, 1)\n",
      "1 : (1, 5)\n",
      "2 : (1, 9)\n",
      "3 : (1, 13)\n",
      "4 : (1, 17)\n",
      "5 : (5, 1)\n",
      "6 : (5, 5)\n",
      "7 : (5, 9)\n",
      "8 : (5, 13)\n",
      "9 : (5, 17)\n",
      "10 : (9, 1)\n",
      "11 : (9, 5)\n",
      "12 : (9, 9)\n",
      "13 : (9, 13)\n",
      "14 : (9, 17)\n",
      "15 : (13, 1)\n",
      "16 : (13, 5)\n",
      "17 : (13, 9)\n",
      "18 : (13, 13)\n",
      "19 : (13, 17)\n",
      "20 : (17, 1)\n",
      "21 : (17, 5)\n",
      "22 : (17, 9)\n",
      "23 : (17, 13)\n",
      "24 : (17, 17)\n"
     ]
    }
   ],
   "source": [
    "for action in range(25):\n",
    "    u1 = (action//5)*4 + 1\n",
    "    u2 = (action%5)*4 + 1\n",
    "    print(action, \":\", (u1, u2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "\n",
    "\n",
    "class yolosystem(gym.Env):\n",
    "    metadata = {'render.modes': ['human']}\n",
    "    \n",
    "    def __init__(self, n_actions, filename):\n",
    "        \n",
    "        super(yolosystem, self).__init__()\n",
    "        \n",
    "        self.n_actions = n_actions #total number of action space after ranging [10, 20, 30 ...]\n",
    "        self.action_space = spaces.Discrete(self.n_actions) #total number of users in the action space; starts with zero\n",
    "        self.observation_space = spaces.Box(low=np.array([0,0,0,0,0,0]), high=np.array([11000]*6), shape=(6, ), dtype=np.int32) #<RAM, Core, Workload>\n",
    "        self.seed()\n",
    "        self.current_obs = np.array( [3000, 2, 40, 2, 1, 1] ) #current observation = <ram, cores, workload%>\n",
    "\n",
    "        #Load dataset\n",
    "        self.df = pd.read_csv(filename)\n",
    "        #data preprocessing step\n",
    "#         self.df['ram'] = self.df['ram'].div(1000).round(0).astype(int)\n",
    "#         self.df['workload_cpu'] = self.df['workload_cpu'].div(10).round(0).astype(int)\n",
    "        self.df['workload_gpu'] = self.df['workload_gpu'].multiply(1/80).round(0).astype(int) #round gpu workload\n",
    "#         self.df['users_yolo'] = self.df['users_yolo'].div(100).round(0).astype(int)\n",
    "#         self.df['users_mnet'] = self.df['users_mnet'].div(100).round(0).astype(int)\n",
    "        \n",
    "        # ... others\n",
    "        #get unique data in set\n",
    "        self.ram = self.df.ram.unique()\n",
    "        self.cores = self.df.cores.unique()\n",
    "        self.workload_cpu = self.df.workload_cpu.unique()\n",
    "        print(self.df)\n",
    "       \n",
    "        \n",
    "\n",
    "    def seed(self, seed=1010):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action) #action should be in action space\n",
    "        state = self.current_obs\n",
    "        done = True #Episodes ends after each action\n",
    "\n",
    "        #compute latecy from the number of users\n",
    "        reward = self.get_reward(state, action) #linear latency           \n",
    "#         print(action, reward)\n",
    "        self.current_obs = self.get_random_state() #go to a random state\n",
    "        \n",
    "#         print(self.current_obs)\n",
    "        return self.current_obs, reward, done, {} #no-states, reward, episode-done, no-info\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_obs = self.get_random_state()\n",
    "        return self.current_obs #current state of the system with no load\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        print(f\"Current State:<{self.current_obs}>\")\n",
    "        \n",
    "    \n",
    "    #compute latency\n",
    "    def get_reward(self, state, action):\n",
    "        #change action to users\n",
    "        \n",
    "        #100 action space\n",
    "#         u1 = (action//10)*2 + 1\n",
    "#         u2 = (action%10)*2 + 1\n",
    "        \n",
    "        #25 action space\n",
    "        u1 = (action//5)*4 + 1\n",
    "        u2 = (action%5)*4 + 1\n",
    "        #sample time from dataframe\n",
    "        gram = state[0]\n",
    "        gcores = state[1]\n",
    "        gwl_c = state[2]\n",
    "        gwl_g = state[3]\n",
    "        gs1 = u1\n",
    "        gs2 = u2\n",
    "#         print(\"user:\", gs1, gs2, \"act:\", action)\n",
    "\n",
    "        \n",
    "        fetch_state = self.df.loc[ (self.df['ram'] == gram) & (self.df['cores']== gcores) & (self.df['workload_cpu']==gwl_c) & (self.df['workload_gpu']==gwl_g) & (self.df['users_yolo']==gs1) & (self.df['users_mnet']==gs2)]\n",
    "        \n",
    "        \n",
    "        if fetch_state.empty:\n",
    "            return -20 #DQN 8\n",
    "#             return 0 #dn 9\n",
    "#         print(fetch_state)\n",
    "        time1 = fetch_state.sample().iloc[0]['time_yolo'] #fetch time from the dataframe\n",
    "        time2 = fetch_state.sample().iloc[0]['time_mnet']\n",
    "        tm = max(time1, time2)\n",
    "        \n",
    "        #compute reward=======================\n",
    "#         print(\"time\", tm)\n",
    "        if (tm <= latency_threshold): # and (gs1 <= state[4]) and (gs2 <= state[5]):\n",
    "#             return 100*np.exp( ( 0.005*(gs1 - state[4]) ) ) + 100*np.exp( ( 0.005*(gs2 - state[5]) ) ) #dqn9\n",
    "#             return 200*np.exp( ( 0.004*(gs1 - state[4]) ) ) + 200*np.exp( ( 0.004*(gs2 - state[5]) ) ) #dqn10\n",
    "#             return -np.exp( ( -0.01*(gs1 - state[4]) ) ) - np.exp( ( -0.01*(gs2 - state[5]) ) )\n",
    "#             return np.exp( ( 0.01*(gs1 - state[4]) ) ) + np.exp( ( 0.01*(gs2 - state[5]) ) ) #DQN6\n",
    "            return  (gs1 - state[4]) +  (gs2 - state[5]) + u1 + u2 #DQN8 (best)\n",
    "#             return  1 + u1 + u2 #DQN7\n",
    "        else:\n",
    "            return - u1 - u2    #DQN 8  \n",
    "#             return 0 #dn9\n",
    "\n",
    "        \n",
    "    \n",
    "    #get to some random state after taking an action\n",
    "    def get_random_state(self):\n",
    "        #generate state randomly\n",
    "        gram = np.random.choice(self.ram, 1)[0]\n",
    "        gcores = np.random.choice(self.cores, 1)[0]\n",
    "        gwl_c = np.random.choice(self.workload_cpu, 1)[0]\n",
    "        \n",
    "        #fetch gamma for the state\n",
    "        fetch_state = self.df.loc[ (self.df['ram'] == gram) & (self.df['cores']== gcores) & (self.df['workload_cpu']==gwl_c) ]\n",
    "        gwl_g = fetch_state.sample().iloc[0]['workload_gpu'] #fetch workload randmoly\n",
    "        \n",
    "        gs1 = random.randrange(1, 20, 5)\n",
    "        gs2 = random.randrange(1, 20, 5)\n",
    "        \n",
    "        return np.array( [gram, gcores, gwl_c, gwl_g, gs1, gs2] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Agent with Baseline3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         ram  cores  workload_cpu  workload_gpu  users_yolo  users_mnet  \\\n",
      "0       3000      2            40             2           1           1   \n",
      "1       3000      2            40             2           1           3   \n",
      "2       3000      2            40             2           1           5   \n",
      "3       3000      2            40             2           1           7   \n",
      "4       3000      2            40             2           1           9   \n",
      "...      ...    ...           ...           ...         ...         ...   \n",
      "23995  11000      5            60            10          19          11   \n",
      "23996  11000      5            60            10          19          13   \n",
      "23997  11000      5            60            10          19          15   \n",
      "23998  11000      5            60            10          19          17   \n",
      "23999  11000      5            60            10          19          19   \n",
      "\n",
      "       time_yolo  time_mnet  \n",
      "0       4.181969   6.505600  \n",
      "1       4.096301   6.710087  \n",
      "2       4.399837   6.695417  \n",
      "3       4.238165   7.089448  \n",
      "4       4.340213   7.089237  \n",
      "...          ...        ...  \n",
      "23995   5.081204   7.736640  \n",
      "23996   5.314953   7.928701  \n",
      "23997   5.162297   8.122567  \n",
      "23998   5.298014   8.079321  \n",
      "23999   5.167603   8.491560  \n",
      "\n",
      "[24000 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "env = yolosystem(25, datafile ) #100 and 25\n",
    "# If the environment don't follow the interface, an error will be thrown\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0, 11000, (6,), int32)\n",
      "Discrete(25)\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "print(env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "10\n",
      "18\n",
      "26\n",
      "34\n",
      "10\n",
      "18\n",
      "26\n",
      "34\n",
      "42\n",
      "18\n",
      "26\n",
      "34\n",
      "42\n",
      "50\n",
      "26\n",
      "34\n",
      "42\n",
      "50\n",
      "58\n",
      "34\n",
      "42\n",
      "50\n",
      "58\n",
      "66\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n",
      "-20\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    t = env.get_reward([3000, 2, 40, 2, 1, 1], i)\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n",
      "reward= 45.0\n",
      "Current State:<[3000.    3.   40.    6.   10.   15.]>\n",
      "Step 2\n",
      "reward= 19.0\n",
      "Current State:<[9000.    3.   60.    3.   11.    2.]>\n",
      "Step 3\n",
      "reward= 23.0\n",
      "Current State:<[11000.     2.    50.     3.    16.    19.]>\n",
      "Step 4\n",
      "reward= -23.0\n",
      "Current State:<[3000.    4.   50.    2.   11.   17.]>\n",
      "Step 5\n",
      "reward= -16.0\n",
      "Current State:<[7000.    2.   40.    2.   18.    6.]>\n",
      "Step 6\n",
      "reward= 36.0\n",
      "Current State:<[5000.    2.   50.   10.    7.   18.]>\n",
      "Step 7\n",
      "reward= 3.0\n",
      "Current State:<[5000.    5.   50.   10.   12.    6.]>\n",
      "Step 8\n",
      "reward= 50.0\n",
      "Current State:<[9000.    2.   40.    3.   14.   11.]>\n",
      "Step 9\n",
      "reward= 43.0\n",
      "Current State:<[11000.     3.    40.    10.    13.    10.]>\n",
      "Step 10\n",
      "reward= 13.0\n",
      "Current State:<[3000.    3.   60.    2.    9.   10.]>\n"
     ]
    }
   ],
   "source": [
    "n_steps = 10\n",
    "\n",
    "for step in range(n_steps):\n",
    "    print(\"Step {}\".format(step + 1))\n",
    "    obs, reward, done, info = env.step(env.action_space.sample())\n",
    "    print('reward=', reward)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import os\n",
    "# Create log dir\n",
    "log_dir = './agent_tensorboard/'\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "env = Monitor(env, log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.dqn import MlpPolicy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# wrap it\n",
    "env = DummyVecEnv([lambda: env])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import A2C\n",
    "# from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "\n",
    "# model = A2C(\"MlpPolicy\", env, verbose=1, tensorboard_log='./agent_tensorboard/')\n",
    "# model.learn(total_timesteps=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(MlpPolicy, env, verbose=0, tensorboard_log = log_dir, exploration_fraction=0.4, learning_starts=10000,  train_freq=30, target_update_interval=5000, exploration_final_eps=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fd273185fa0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=30000) #reset_num_timesteps=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the agent\n",
    "model.save(f\"edge_agent_action\")\n",
    "# model.save(f\"edge_agent_{latency_threshold}_lin\")\n",
    "# del model  # delete trained model to demonstrate loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "# from stable_baselines3 import DQN\n",
    "# model = DQN.load(\"edge_agent_action_threat\")\n",
    "#return action and state\n",
    "#model.predict(np.array([2000, 4, 30]), deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# begin = time.time()\n",
    "# model.predict(np.array([3, 4, 5, 2, 5, 5]), deterministic=True)\n",
    "# end = time.time()\n",
    "# t = end-begin\n",
    "# print(f\"{t}seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "act = model.predict(np.array([3000, 2, 40, 2, 4, 4]), deterministic=True)[0]\n",
    "print(act)\n",
    "u1 = (act//10)*2 + 1\n",
    "u2 = (act%10)*2 + 1\n",
    "print(u1, u2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
